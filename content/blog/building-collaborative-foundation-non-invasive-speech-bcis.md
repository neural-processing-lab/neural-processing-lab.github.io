---
title: "Building a Collaborative Foundation for Non-Invasive Speech BCIs: The 2025 PNPL Competition"
excerpt: "Exploring the motivation behind the 2025 PNPL Competition and how we're building towards non-invasive speech brain-computer interfaces through deep MEG datasets and collaborative research."
author: "Gereon Elvers, Gilad Landau, Dulhan Jayalath, Francesco Mantegna, Oiwi Parker Jones"
date: "2025-01-20"
tags: ["Brain-Computer Interfaces", "MEG", "Speech Decoding", "LibriBrain", "Competition"]
selfCitation: "@misc{pnpl_blog2025collaboration,\n  title={Building a Collaborative Foundation for Non-Invasive Speech {BCIs}: The 2025 {PNPL} Competition},\n  author={Elvers, Gereon and Landau, Gilad and Jayalath, Dulhan and Mantegna, Francesco and Parker Jones, Oiwi},\n  year={2025},\n  url={https://neural-processing-lab.github.io/2025-libribrain-competition/blog/building-collaborative-foundation-non-invasive-speech-bcis/},\n  note={Blog post}\n}"
citations:
  - id: "metzger2023generalizable"
    title: "A high-performance neuroprosthesis for speech decoding and avatar control"
    authors: ["Sean L. Metzger", "Kaylo T. Littlejohn", "Alexander B. Silva", "David A. Moses", "Margaret P. Seaton", "Ran Wang", "Maximilian E. Dougherty", "Jennifer L. Collinger", "Michael L. Boninger", "Edward F. Chang"]
    journal: "Nature"
    year: 2023
    volume: "620"
    pages: "1031-1036"
    doi: "10.1038/s41586-023-06443-4"
    bibtex: "@article{metzger2023generalizable, title={A high-performance neuroprosthesis for speech decoding and avatar control}, author={Metzger, Sean L and Littlejohn, Kaylo T and Silva, Alexander B and Moses, David A and Seaton, Margaret P and Wang, Ran and Dougherty, Maximilian E and Collinger, Jennifer L and Boninger, Michael L and Chang, Edward F}, journal={Nature}, volume={620}, number={7976}, pages={1031--1036}, year={2023}, publisher={Nature Publishing Group}}"
  - id: "willett2023speech"
    title: "A high-performance speech neuroprosthesis"
    authors: ["Francis R. Willett", "Erin M. Kunz", "Chaofei Fan", "Donald T. Avansino", "Guy H. Wilson", "Eun Young Choi", "Foram Kamdar", "Matthew F. Glasser", "Leigh R. Hochberg", "Shaul Druckmann", "Krishna V. Shenoy", "Jaimie M. Henderson"]
    journal: "Nature"
    year: 2023
    volume: "620"
    pages: "1031-1036"
    url: "https://www.nature.com/articles/s41586-023-06377-x"
    bibtex: "@article{willett2023speech, title={A high-performance speech neuroprosthesis}, author={Willett, Francis R and Kunz, Erin M and Fan, Chaofei and Avansino, Donald T and Wilson, Guy H and Choi, Eun Young and Kamdar, Foram and Glasser, Matthew F and Hochberg, Leigh R and Druckmann, Shaul and others}, journal={Nature}, volume={620}, number={7976}, pages={1031--1036}, year={2023}, publisher={Nature Publishing Group}}"
  - id: "jayalath2025unlocking"
    title: "Unlocking non-invasive brain-to-text"
    authors: ["Dulhan Jayalath", "Gilad Landau", "Oiwi Parker Jones"]
    journal: "arXiv preprint"
    year: 2025
    url: "https://arxiv.org/abs/2505.13446"
    bibtex: "@article{jayalath2025unlocking, title={Unlocking non-invasive brain-to-text}, author={Jayalath, Dulhan and Landau, Gilad and Parker Jones, Oiwi}, journal={arXiv preprint arXiv:2505.13446}, year={2025}}"
  - id: "ozdogan2025libribrain"
    title: "LibriBrain: Over 50 hours of within-subject MEG to improve speech decoding methods at scale"
    authors: ["Miran Özdogan", "Gilad Landau", "Gereon Elvers", "Dulhan Jayalath", "Pranav Somaiya", "Francesco Mantegna", "Mark Woolrich", "Oiwi Parker Jones"]
    journal: "arXiv preprint"
    year: 2025
    url: "https://www.arxiv.org/abs/2506.02098"
    bibtex: "@article{ozdogan2025libribrain, title={LibriBrain: Over 50 hours of within-subject MEG to improve speech decoding methods at scale}, author={Özdogan, Miran and Landau, Gilad and Elvers, Gereon and Jayalath, Dulhan and Somaiya, Pranav and Mantegna, Francesco and Woolrich, Mark and Parker Jones, Oiwi}, journal={arXiv preprint arXiv:2506.02098}, year={2025}}"
  - id: "moses2021neuroprosthesis"
    title: "Neuroprosthesis for decoding speech in a paralyzed person with anarthria"
    authors: ["David A. Moses", "Sean L. Metzger", "Jessie R. Liu", "Gopala K. Anumanchipalli", "Joseph G. Makin", "Pengfei F. Sun", "Josh Chartier", "Maximilian E. Dougherty", "Patricia M. Liu", "Gary M. Abrams", "Adelyn Tu-Chan", "Karunesh Ganguly", "Edward F. Chang"]
    journal: "New England Journal of Medicine"
    year: 2021
    volume: "385"
    pages: "217-227"
    doi: "10.1056/NEJMoa2027540"
    bibtex: "@article{moses2021neuroprosthesis, title={Neuroprosthesis for decoding speech in a paralyzed person with anarthria}, author={Moses, David A and Metzger, Sean L and Liu, Jessie R and Anumanchipalli, Gopala K and Makin, Joseph G and Sun, Pengfei F and Chartier, Josh and Dougherty, Maximilian E and Liu, Patricia M and Abrams, Gary M and others}, journal={New England Journal of Medicine}, volume={385}, number={3}, pages={217--227}, year={2021}, publisher={Mass Medical Soc}}"
  - id: "kaplan2020scaling"
    title: "Scaling laws for neural language models"
    authors: ["Jared Kaplan", "Sam McCandlish", "Tom Henighan", "Tom B. Brown", "Benjamin Chess", "Rewon Child", "Scott Gray", "Alec Radford", "Jeffrey Wu", "Dario Amodei"]
    journal: "arXiv preprint"
    year: 2020
    url: "https://arxiv.org/abs/2001.08361"
    bibtex: "@article{kaplan2020scaling, title={Scaling laws for neural language models}, author={Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario}, journal={arXiv preprint arXiv:2001.08361}, year={2020}}"
  - id: "defossez2023decoding"
    title: "Decoding speech perception from non-invasive brain recordings"
    authors: ["Alexandre Défossez", "Charlotte Caucheteux", "Jérémy Rapin", "Ori Kabeli", "Jean-Rémi King"]
    journal: "Nature Machine Intelligence"
    year: 2023
    volume: "5"
    pages: "1097-1107"
    url: "https://www.nature.com/articles/s42256-023-00714-5"
    bibtex: "@article{defossez2023decoding, title={Decoding speech perception from non-invasive brain recordings}, author={Défossez, Alexandre and Caucheteux, Charlotte and Rapin, Jérémy and Kabeli, Ori and King, Jean-Rémi}, journal={Nature Machine Intelligence}, volume={5}, number={10}, pages={1097--1107}, year={2023}, publisher={Nature Publishing Group}}"
  - id: "tang2023semantic"
    title: "Semantic reconstruction of continuous language from non-invasive brain recordings"
    authors: ["Jerry Tang", "Amanda LeBel", "Shailee Jain", "Alexander G. Huth"]
    journal: "Nature Neuroscience"
    year: 2023
    volume: "26"
    pages: "858-866"
    url: "https://www.nature.com/articles/s41593-023-01304-9"
    bibtex: "@article{tang2023semantic, title={Semantic reconstruction of continuous language from non-invasive brain recordings}, author={Tang, Jerry and LeBel, Amanda and Jain, Shailee and Huth, Alexander G}, journal={Nature Neuroscience}, volume={26}, number={5}, pages={858--866}, year={2023}, publisher={Nature Publishing Group}}"
  - id: "landau2025pnpl"
    title: "The 2025 PNPL competition: Speech detection and phoneme classification in the LibriBrain dataset"
    authors: ["Gilad Landau", "Miran Özdogan", "Gereon Elvers", "Francesco Mantegna", "Pranav Somaiya", "Dulhan Jayalath", "Lukas Kurth", "Taewon Kwon", "Brendan Shillingford", "Gabriel Farquhar", "Michael Jiang", "Karim Jerbi", "Hichem Abdelhedi", "Yorguin Mantilla Ramos", "Caglar Gulcehre", "Mark Woolrich", "Natalie Voets", "Oiwi Parker Jones"]
    journal: "NeurIPS Competition Track"
    year: 2025
    url: "https://arxiv.org/abs/2506.10165"
    bibtex: "@article{landau2025pnpl, title={The 2025 PNPL competition: Speech detection and phoneme classification in the LibriBrain dataset}, author={Landau, Gilad and Özdogan, Miran and Elvers, Gereon and Mantegna, Francesco and Somaiya, Pranav and Jayalath, Dulhan and Kurth, Lukas and Kwon, Taewon and Shillingford, Brendan and Farquhar, Gabriel and others}, journal={arXiv preprint arXiv:2506.10165}, year={2025}}"
---

> **Note:** This blog post was originally published on the [LibriBrain Competition website](http://libribrain.com/) and is reproduced here with permission.

The field of brain-computer interfaces (BCIs) has reached an exciting inflection point. Recent breakthroughs in invasive speech BCIs have achieved remarkable milestones—[reported word-error rates below 5%](https://www.nejm.org/doi/full/10.1056/NEJMoa2314132) and [vocabularies exceeding 125,000 words](https://www.nature.com/articles/s41586-023-06377-x). But these advances come with a fundamental limitation: they require brain surgery.

<div style="position: relative; display: inline-block; cursor: pointer; width: 100%;" onclick="this.querySelector('.blur-overlay').style.display='none'; this.style.cursor='default';">
  <img src="/blog/blog1-building-collaborative-foundation-non-invasive-speech-bcis/blog1-picture1.png" alt="Brain surgery procedure" style="width: 100%; height: auto;" />
  <div class="blur-overlay" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; backdrop-filter: blur(20px); background: rgba(0,0,0,0.3); display: flex; align-items: center; justify-content: center; color: white; font-weight: bold; text-align: center; padding: 20px; box-sizing: border-box;">
    ⚠️ Click to view surgical image<br/>(Warning: contains blood and medical content)
  </div>
</div>

*Part of the surgery to implant invasive electrodes into the brain (from [Wikipedia](https://en.wikipedia.org/wiki/Craniotomy#/media/File:Craniotomy_Arachnoid.jpg))*

What if we could achieve similar breakthroughs without surgical intervention? This is the driving vision behind the 2025 PNPL Competition, and we're excited to share why we believe this competition represents a critical step towards that future.

## The Ultimate Goal: Speech BCIs Without Surgery

Our ultimate goal is clear: developing speech brain-computer interfaces that don't require surgical implantation. The potential impact is profound—from restoring communication for individuals with paralysis or speech deficits like dysarthria, to enabling new forms of human-computer interaction that could benefit everyone.

However, non-invasive speech decoding faces significant challenges. While invasive BCIs leverage high signal-to-noise ratios from electrodes placed directly on or in the brain, non-invasive methods must contend with signals that are attenuated by the distance from brain and sensor. The gap between invasive and non-invasive performance has been substantial—[most current non-invasive approaches show word-error rates approaching 100%](https://arxiv.org/abs/2405.06459) (though see [Jayalath et al.](https://arxiv.org/abs/2505.13446)), whilst invasive systems report below 5%.

## Addressing Field Limitations with LibriBrain

To tackle these challenges head-on, we've created [LibriBrain](https://huggingface.co/datasets/pnpl/LibriBrain): the largest within-subject MEG dataset recorded to date, containing over 50 hours of high-quality 306-channel neural recordings sampled at 250 Hz. This represents a 5× increase over the next largest dataset and 25-50× more data than typical MEG/EEG datasets.

![LibriBrain dataset comparison](/blog/blog1-building-collaborative-foundation-non-invasive-speech-bcis/blog1-picture2.png)

*Figure reproduced from [Özdogan et al](https://www.arxiv.org/abs/2506.02098).*

The dataset spans nearly the entire Sherlock Holmes canon across 7 books, recorded over 95 sessions from a single participant. This depth matters enormously—our baseline experiments demonstrate clear logarithmic scaling relationships between training data volume and decoding performance, consistent with scaling laws observed across machine learning domains.

Why does this matter? Empirical evidence shows that "deep data"—extensive recordings from the same individual—yields the largest gains in decoding performance. Previous datasets have been characterised as "broad but shallow," including many participants but only 1-2 hours per person. LibriBrain flips this paradigm, providing the depth needed to train powerful AI models that can compete with surgical alternatives. That said, as we continue to grow the dataset, we also plan in future to release data from additional subjects.

## Competition Design: Building Strong Foundations

### Why These Tasks?

We've chosen two foundational tasks for this competition: Speech Detection and Phoneme Classification. Whilst our ultimate goal is brain-to-text decoding, recent attempts at full sentence decoding from non-invasive signals have yielded near-chance performance.

Instead of jumping directly to the hardest problem, we're taking inspiration from the development of automatic speech recognition (ASR), which built strong foundations through phoneme-based approaches. Here's why these tasks make sense:

Speech Detection provides a binary classification for every temporal sample (250 per second), making it our most accessible task—maximum data points with the smallest number of classes, [compared for example to word or even phoneme events](https://www.arxiv.org/abs/2506.02098). This entry-level challenge mirrors the critical speech detection component used in the [first invasive speech BCI](https://www.nejm.org/doi/full/10.1056/NEJMoa2027540) for paralysed individuals.

![Speech detection visualization](/blog/blog1-building-collaborative-foundation-non-invasive-speech-bcis/blog1-picture3.gif)

Phoneme Classification offers several advantages over word-level tasks:

* **No out-of-vocabulary problems**: Unlike word classification, every phoneme in test data has been seen during training
* **Better data efficiency**: 1.5M phoneme examples across 39 classes versus 466k words across 16k+ classes  
* **Strong theoretical foundation**: Phonemes are the building blocks of speech

![Phoneme classification visualization](/blog/blog1-building-collaborative-foundation-non-invasive-speech-bcis/blog1-picture4.gif)

Our baseline results show promising scaling behaviour which mirrors neural scaling laws observed across AI domains (e.g. [Kaplan et al.](https://arxiv.org/abs/2001.08361) and [Antonello et al.](https://proceedings.neurips.cc/paper_files/paper/2023/hash/4533e4a352440a32558c1c227602c323-Abstract-Conference.html)). Both speech detection and phoneme classification performance improve logarithmically with the volume of training data:

![Scaling results](/blog/blog1-building-collaborative-foundation-non-invasive-speech-bcis/blog1-picture5.png)

*Figure reproduced from [Özdogan et al](https://www.arxiv.org/abs/2506.02098). (x-axes are log scale)*

### Why MEG?

Magnetoencephalography (MEG) offers unique advantages that make it ideal for bridging the gap between invasive and practical non-invasive BCIs:

* **Direct measurement of neural activity** with millisecond temporal resolution
* **Minimal signal distortion** compared to EEG, as magnetic fields pass through tissue without degradation  
* **Spatial precision** of 5-10mm routinely, with 2mm achieved in some studies
* **No surgical risk**, enabling safe data collection at unprecedented scale

Recent work by [Défossez et al.](https://www.nature.com/articles/s42256-023-00714-5), [d'Ascoli et al.](https://arxiv.org/abs/2412.17829), and [Jayalath et al.](https://arxiv.org/abs/2406.04328) has demonstrated the potential of scaling non-invasive approaches with large datasets and modern AI methods. Whilst practical applications may eventually move towards more portable technologies like EEG, MEG provides the best current platform for proving that non-invasive speech decoding can compete with surgical alternatives.

![MEG scanner](/blog/blog1-building-collaborative-foundation-non-invasive-speech-bcis/blog1-picture6.png)
*MEG provides high-quality, non-invasive brain recordings ([Oxford scanner](https://www.win.ox.ac.uk/research/research-facilities/magnetoencephalography) used for LibriBrain)*

### Why Speech Comprehension?

Our competition focuses on decoding speech comprehension (listening) rather than speech production. This design choice reflects both practical and strategic considerations:

**Methodological advantages**: Speech comprehension allows us to develop scalable AI methods with precise event timing. We know exactly when each phoneme occurs, enabling reliable model training and evaluation. Comprehension tasks further avoid muscle artifacts introduced by speech production (e.g. speaking aloud) which reduce signal quality and confound non-invasive decoding.

**Alignment with literature**: This approach aligns with recent successful non-invasive decoding work, including influential papers by [Tang et al.](https://www.nature.com/articles/s41593-023-01304-9), [Défossez et al.](https://www.nature.com/articles/s42256-023-00714-5), and our own recent advances in non-invasive brain-to-text (e.g. [Jayalath et al.](https://arxiv.org/abs/2406.04328) and [Jayalath et al.](https://arxiv.org/abs/2505.13446)) that show promise for generalising from comprehension to production tasks.

**Stepping stone to production**: Whilst we ultimately want to decode attempted speech or inner speech, comprehension provides a crucial intermediate step for developing and validating methods that can later generalise to production tasks. This approach is supported by evidence of overlap in the brain between speech comprehension and production representations (e.g. [Hickok and Poeppel](https://www.nature.com/articles/nrn2113) and [Pulvermüller et al.](https://www.pnas.org/doi/10.1073/pnas.0509989103)), and by recent evidence that decoding models trained on listening data can transfer to inner speech tasks (e.g. [Tang et al.](https://www.nature.com/articles/s41593-023-01304-9)).

## Driving Innovation and Building Community

This competition is designed to create an "ImageNet moment" for non-invasive neural decoding, inspired by how [ImageNet](https://www.image-net.org/) revolutionised computer vision. We're providing not just data, but [a complete ecosystem designed around the foundational tasks we've established](https://arxiv.org/abs/2506.10165):

* **Standardised evaluation metrics** to enable fair comparisons
* **Easy-to-use Python library (pnpl)** with PyTorch integration, designed to be as accessible as CIFAR-10
* **Two competition tracks**: Standard (LibriBrain data only) and Extended (any training data)
* **Tutorial materials** that run in free Google Colab environments
* **Active community support** through Discord and dedicated resources
* **Public leaderboards** for real-time progress tracking

The dual-track structure ensures both algorithmic innovation (Standard track) and exploration of what's possible with unlimited compute resources (Extended track). At least £10,000 in prizes will reward the most innovative approaches.

## Looking Forward

This is just the beginning. The hope is that the PNPL Competition will become an annual event, tackling a progression of increasingly challenging tasks as our datasets grow and our methods and insights improve. Think of it as a curriculum for the field—building from strong foundations towards the ultimate goal of robust, non-invasive speech BCIs.

We'll be releasing a series of blog posts throughout the competition to dive deeper into technical aspects, share insights from the community, and highlight innovative approaches. Whether you're a seasoned researcher or new to the field, we've designed this competition to be accessible and impactful.

The future of speech BCIs doesn't have to require surgery—let's work together to make this future a reality

---

***Ready to participate? Visit the rest of our [competition website](https://neural-processing-lab.github.io/2025-libribrain-competition/) to get started with tutorials, download the data, and join our community [Discord](https://neural-processing-lab.github.io/2025-libribrain-competition/links/discord). The competition is officially running from 1st June 2025 - 30 September 2025 and will culminate in a session at NeurIPS in December 2025. As a record of progress, we expect the submission system and leaderboards to continue running afterward.***

---